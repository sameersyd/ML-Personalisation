{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textsearch in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (0.0.17)\n",
      "Requirement already satisfied: Unidecode in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch) (1.1.1)\n",
      "Requirement already satisfied: pyahocorasick in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch) (1.4.0)\n",
      "Requirement already satisfied: contractions in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (0.0.24)\n",
      "Requirement already satisfied: textsearch in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: Unidecode in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\n",
      "Requirement already satisfied: pyahocorasick in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sameernawaz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sameernawaz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install textsearch\n",
    "!pip install contractions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['airlines', 'celebrities', 'colleges', 'fastfood', \n",
    "              'leagues', 'news', 'streamingplatforms', 'techgiants']\n",
    "\n",
    "subCategories = [['output_AmericanAir', 'output_JetBlue', 'output_qatarairways', 'output_SingaporeAir'], \n",
    "                 ['output_TheRock', 'output_ArianaGrande', 'output_chrissyteigen', 'output_jimmyfallon'], \n",
    "                 ['output_Stanford', 'output_Harvard', 'output_MIT', 'output_UCBerkeley'], \n",
    "                 ['output_McDonalds', 'output_BurgerKing', 'output_Starbucks', 'output_wendys'], \n",
    "                 ['output_IPL', 'output_MLB', 'output_MLS', 'output_NBA'], \n",
    "                 ['output_BBCWorld', 'output_CNN', 'output_washingtonpost', 'output_nytimes'], \n",
    "                 ['output_Netflix', 'output_Spotify', 'output_PrimeVideo', 'output_HBO'], \n",
    "                 ['output_amazon', 'output_facebook', 'output_Tesla', 'output_Microsoft']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800\n"
     ]
    }
   ],
   "source": [
    "sizeOfSets = 400\n",
    "\n",
    "categCount = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in subCategories[i]:\n",
    "        categCount += 1\n",
    "        \n",
    "categCount *= sizeOfSets\n",
    "\n",
    "val = [None] * categCount # Keep value (sizeOfSets * no. of categories)\n",
    "sizeOfTweetContent = 75\n",
    "\n",
    "print(len(val)) # No. Of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputDataInDataframe():\n",
    "    overCount= 0\n",
    "    for i in range(len(categories)):\n",
    "        for j in subCategories[i]:\n",
    "            direc:str = '/Users/sameernawaz/AnacondaProjects/Datasets/tweets/'+categories[i]+'/'+j+'.csv'\n",
    "            with open(direc, 'r') as csv_file:\n",
    "                csv_reader = csv.reader(csv_file)\n",
    "                count = 0\n",
    "                for line in csv_reader:\n",
    "                    if len(line[6]) > sizeOfTweetContent:\n",
    "                        val[overCount] = line\n",
    "                        count += 1\n",
    "                        overCount += 1\n",
    "                        if count >= sizeOfSets :\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataInDataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(val, columns = ['date', 'username', 'to', 'replies', \n",
    "                                  'retweets', 'favorites', 'text', 'geo', \n",
    "                                  'mentions', 'hashtags', 'id', 'permalink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once data added in dataframe, Shuffle it\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# Add a unique id (rec_id) for all tweets\n",
    "df = df.assign(rec_id=np.arange(len(df))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12800 entries, 0 to 12799\n",
      "Data columns (total 5 columns):\n",
      "rec_id      12800 non-null int64\n",
      "username    12800 non-null object\n",
      "text        12800 non-null object\n",
      "date        12800 non-null object\n",
      "retweets    12800 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 600.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df[['rec_id', 'username', 'text', 'date', 'retweets']]\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    doc = contractions.fix(doc)\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    #filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(list(df['text']))\n",
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12800, 26984)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Pairwise Document Similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)\n",
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.iloc[11351, 12423]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     1,     2, ..., 12797, 12798, 12799]), (12800,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of post IDs\n",
    "\n",
    "posts_list = df['rec_id'].values\n",
    "posts_list, posts_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11351"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_idx = np.where(posts_list == 11351)[0][0]\n",
    "posts_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_similarities = doc_sim_df.iloc[posts_idx].values\n",
    "posts_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11630,  3469, 12791,  5389, 11049])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 5 similar post IDs\n",
    "\n",
    "similar_posts_idxs = np.argsort(-posts_similarities)[1:6]\n",
    "similar_posts_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11630,  3469, 12791,  5389, 11049])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_posts = posts_list[similar_posts_idxs]\n",
    "similar_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_posts(postId, returnSize, posts = posts_list, doc_sims = doc_sim_df):\n",
    "    # find post id\n",
    "    posts_idx = np.where(posts == postId)[0][0]\n",
    "    # get posts similarities\n",
    "    posts_similarities = doc_sims.iloc[posts_idx].values\n",
    "    # get top 5 similar post IDs\n",
    "    similar_posts_idxs = np.argsort(-posts_similarities)[1:returnSize+1]\n",
    "    # get top 5 posts\n",
    "    similar_posts = posts[similar_posts_idxs]\n",
    "    # return the top 5 posts\n",
    "    return similar_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Input Post-----\n",
      "With #MLSCup in their sights, @LAFC turn their attention to Seattle: http://soc.cr/h9bu50wUB4y #LAFC\n",
      "\n",
      "-----Relevant Posts-----\n",
      "=> #LAFC #LAGalaxy. Meant to be. @LAFC relishing the chance to take down their cross-town rivals in the Audi #MLSCupPlayoffs: http://soc.cr/LePJ50wRKGv\n",
      "=> âˆ™ Kelyn Rowe Seattle âˆ™ Juan Agudelo LAFC @MattDoyle76 breaks down the moves he ð‘¤ð‘Žð‘›ð‘¡ð‘  to see. http://soc.cr/Z1Mz50xksGE\n",
      "=> A sellout in Seattle? You betcha! http://soc.cr/eoNe50x09zd #MLSCup // : @SoundersFC\n",
      "=> Will @LAFC continue their historic season and make it to #MLSCup? #LAFCvSEA // Audi #MLSCupPlayoffs\n",
      "=> They met twice in the regular season... #LAFC 4-1 #Sounders #Sounders 1-1 #LAFC Our guys break down those meetings and look ahead to tonight. #LAFCvSEA // Audi #MLSCupPlayoffs\n",
      "=>  Single-season scoring record Supporters' Shield MVP #MLSCup It's been a magical season for @11carlosV and @lafc.\n"
     ]
    }
   ],
   "source": [
    "post_id = 11323\n",
    "returnValSize = 6\n",
    "\n",
    "print('-----Input Post-----')\n",
    "print((df.loc[df['rec_id'] == post_id]['text']).values[0])\n",
    "print('\\n-----Relevant Posts-----')\n",
    "for i in relevant_posts(postId = post_id, returnSize = returnValSize, posts = posts_list, doc_sims = doc_sim_df):\n",
    "    print(\"=>\",(df.loc[df['rec_id'] == i]['text']).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Input Post-----\n",
      "Analysis: The latest Ukraine revelation spotlights a major gap in Trumpâ€™s defense\n",
      "\n",
      "-----Relevant Posts-----\n"
     ]
    }
   ],
   "source": [
    "# Sort Posts\n",
    "\n",
    "post_id = 90\n",
    "returnValSize = 15\n",
    "\n",
    "postsList = []\n",
    "\n",
    "print('-----Input Post-----')\n",
    "print((df.loc[df['rec_id'] == post_id]['text']).values[0])\n",
    "print('\\n-----Relevant Posts-----')\n",
    "for i in relevant_posts(postId = post_id, returnSize = returnValSize, posts = posts_list, doc_sims = doc_sim_df):\n",
    "    postsList.append((df.loc[df['rec_id'] == i]).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Perspective: Trumpâ€™s xenophobia is an American tradition â€” but it doesnâ€™t have to be\n",
      "=> Analysis | Choose your own adventure: Can you keep from being impeached over Ukraine\n",
      "=> Analysis: The unmitigated ludicrousness of Trump trying to distance himself from Giulianiâ€™s work on Ukraine\n",
      "=> Analysis: 3 takeaways from Mark Sandyâ€™s and Philip Reekerâ€™s testimony on Ukraine\n",
      "=> Opinion: Pompeo just flirted with Trumpâ€™s Ukraine conspiracy theory. This isnâ€™t normal, folks.\n"
     ]
    }
   ],
   "source": [
    "def sortDate(val): \n",
    "    return val[3]\n",
    "\n",
    "def sortRetweet(val): \n",
    "    return val[4]\n",
    "\n",
    "postsList.sort(key=sortDate, reverse=True)\n",
    "\n",
    "recentList = postsList[0:5]\n",
    "\n",
    "for i in recentList:\n",
    "    print(\"=>\", i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
