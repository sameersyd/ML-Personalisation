{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['airlines', 'celebrities', 'colleges', 'fastfood', \n",
    "              'leagues', 'news', 'streamingplatforms', 'techgiants']\n",
    "\n",
    "subCategories = [['output_AmericanAir', 'output_JetBlue', 'output_qatarairways', 'output_SingaporeAir'], \n",
    "                 ['output_TheRock', 'output_ArianaGrande', 'output_chrissyteigen', 'output_jimmyfallon'], \n",
    "                 ['output_Stanford', 'output_Harvard', 'output_MIT', 'output_UCBerkeley'], \n",
    "                 ['output_McDonalds', 'output_BurgerKing', 'output_Starbucks', 'output_wendys'], \n",
    "                 ['output_IPL', 'output_MLB', 'output_MLS', 'output_NBA'], \n",
    "                 ['output_BBCWorld', 'output_CNN', 'output_washingtonpost', 'output_nytimes'], \n",
    "                 ['output_Netflix', 'output_Spotify', 'output_PrimeVideo', 'output_HBO'], \n",
    "                 ['output_amazon', 'output_facebook', 'output_Tesla', 'output_Microsoft']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800\n"
     ]
    }
   ],
   "source": [
    "sizeOfSets = 400\n",
    "\n",
    "categCount = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in subCategories[i]:\n",
    "        categCount += 1\n",
    "        \n",
    "categCount *= sizeOfSets\n",
    "\n",
    "val = [None] * categCount # Keep value (sizeOfSets * no. of categories)\n",
    "sizeOfTweetContent = 75\n",
    "\n",
    "print(len(val)) # No. Of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputDataInDataframe():\n",
    "    overCount= 0\n",
    "    for i in range(len(categories)):\n",
    "        for j in subCategories[i]:\n",
    "            direc:str = '/Users/sameernawaz/AnacondaProjects/Datasets/tweets/'+categories[i]+'/'+j+'.csv'\n",
    "            with open(direc, 'r') as csv_file:\n",
    "                csv_reader = csv.reader(csv_file)\n",
    "                count = 0\n",
    "                for line in csv_reader:\n",
    "                    if len(line[6]) > sizeOfTweetContent:\n",
    "                        val[overCount] = line\n",
    "                        count += 1\n",
    "                        overCount += 1\n",
    "                        if count >= sizeOfSets :\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataInDataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(val, columns = ['date', 'username', 'to', 'replies', \n",
    "                                  'retweets', 'favorites', 'text', 'geo', \n",
    "                                  'mentions', 'hashtags', 'id', 'permalink']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once data added in dataframe, Shuffle it\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# Add a unique id (rec_id) for all tweets\n",
    "df = df.assign(rec_id=np.arange(len(df))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12800 entries, 0 to 12799\n",
      "Data columns (total 3 columns):\n",
      "rec_id      12800 non-null int64\n",
      "username    12800 non-null object\n",
      "text        12800 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 400.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df[['rec_id', 'username', 'text']]\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    doc = contractions.fix(doc)\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    #filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(list(df['text']))\n",
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12800, 26984)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12790</th>\n",
       "      <th>12791</th>\n",
       "      <th>12792</th>\n",
       "      <th>12793</th>\n",
       "      <th>12794</th>\n",
       "      <th>12795</th>\n",
       "      <th>12796</th>\n",
       "      <th>12797</th>\n",
       "      <th>12798</th>\n",
       "      <th>12799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4      5      6      7      8      \\\n",
       "0    1.0  0.000000  0.000000  0.000000  0.000000    0.0    0.0    0.0    0.0   \n",
       "1    0.0  1.000000  0.000000  0.054008  0.000000    0.0    0.0    0.0    0.0   \n",
       "2    0.0  0.000000  1.000000  0.000000  0.013809    0.0    0.0    0.0    0.0   \n",
       "3    0.0  0.054008  0.000000  1.000000  0.000000    0.0    0.0    0.0    0.0   \n",
       "4    0.0  0.000000  0.013809  0.000000  1.000000    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      9      ...  12790  12791  12792  12793     12794  12795  12796  12797  \\\n",
       "0  0.000000  ...    0.0    0.0    0.0    0.0  0.045305    0.0    0.0    0.0   \n",
       "1  0.027313  ...    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "2  0.000000  ...    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3  0.000000  ...    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "4  0.000000  ...    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "\n",
       "   12798  12799  \n",
       "0    0.0    0.0  \n",
       "1    0.0    0.0  \n",
       "2    0.0    0.0  \n",
       "3    0.0    0.0  \n",
       "4    0.0    0.0  \n",
       "\n",
       "[5 rows x 12800 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Pairwise Document Similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)\n",
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     1,     2, ..., 12797, 12798, 12799]), (12800,))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of post IDs\n",
    "\n",
    "posts_list = df['rec_id'].values\n",
    "posts_list, posts_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3997"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_idx = np.where(posts_list == 3997)[0][0]\n",
    "posts_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_similarities = doc_sim_df.iloc[posts_idx].values\n",
    "posts_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4535,  8247,  7841,  4486, 11595])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 5 similar post IDs\n",
    "\n",
    "similar_posts_idxs = np.argsort(-posts_similarities)[1:6]\n",
    "similar_posts_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4535,  8247,  7841,  4486, 11595])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_posts = posts_list[similar_posts_idxs]\n",
    "similar_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_posts(postId, posts = posts_list, doc_sims = doc_sim_df):\n",
    "    # find post id\n",
    "    posts_idx = np.where(posts == postId)[0][0]\n",
    "    # get posts similarities\n",
    "    posts_similarities = doc_sims.iloc[posts_idx].values\n",
    "    # get top 5 similar post IDs\n",
    "    similar_posts_idxs = np.argsort(-posts_similarities)[1:6]\n",
    "    # get top 5 posts\n",
    "    similar_posts = posts[similar_posts_idxs]\n",
    "    # return the top 5 posts\n",
    "    return similar_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Input Post-----\n",
      "More #SeptemberBaseball on @Youtube! Watch @RaysBaseball vs. the @Dodgers tomorrow at 10:00pm ET. Set a reminder for a game time notification https://bit.ly/2kBd2Zn\n",
      "\n",
      "-----Relevant Posts-----\n",
      "=> A game with Postseason implications! Watch @Indians-@Angles tomorrow at 10:00pm ET LIVE on @YouTube. Don't forget to set a YouTube reminder to get notified at game time http://bit.ly/2k3oDzQ\n",
      "=> An NL East showdown between postseason contenders. Watch @Braves-@Nationals tomorrow at 7 pm ET on @YouTube. Set a reminder for game time: https://atmlb.com/31fvsi8\n",
      "=> #SeptemberBaseball on @YouTube! It’s @Mets - @Nationals tomorrow at 1:00pm. Don't forget to set a reminder to get notified at game time. https://bit.ly/2k1Lk7I\n",
      "=>  FREE @RaysBaseball are taking on the @Dodgers LIVE on @YouTube today at 10:00pm ET. Catch the action https://atmlb.com/2ZZvcXn\n",
      "=> We have more free baseball tomorrow on @YouTube! Catch the @BlueJays - @RaysBaseball live at 7:00pm ET. Don't forget to set a reminder and get notified at game time. https://bit.ly/2krqEpQ\n"
     ]
    }
   ],
   "source": [
    "post_id = 10059\n",
    "\n",
    "print('-----Input Post-----')\n",
    "print((df.loc[df['rec_id'] == post_id]['text']).values[0])\n",
    "print('\\n-----Relevant Posts-----')\n",
    "for i in relevant_posts(postId = post_id, posts = posts_list, doc_sims = doc_sim_df):\n",
    "    print(\"=>\",(df.loc[df['rec_id'] == i]['text']).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
