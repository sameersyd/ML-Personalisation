{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textsearch in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (0.0.17)\n",
      "Requirement already satisfied: pyahocorasick in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch) (1.1.1)\n",
      "Requirement already satisfied: contractions in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (0.0.24)\n",
      "Requirement already satisfied: textsearch in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: Unidecode in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\n",
      "Requirement already satisfied: pyahocorasick in /Users/sameernawaz/opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sameernawaz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sameernawaz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install textsearch\n",
    "!pip install contractions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['airlines', 'celebrities', 'colleges', 'fastfood', \n",
    "              'leagues', 'news', 'streamingplatforms', 'techgiants']\n",
    "\n",
    "subCategories = [['output_AmericanAir', 'output_JetBlue', 'output_qatarairways', 'output_SingaporeAir'], \n",
    "                 ['output_TheRock', 'output_ArianaGrande', 'output_chrissyteigen', 'output_jimmyfallon'], \n",
    "                 ['output_Stanford', 'output_Harvard', 'output_MIT', 'output_UCBerkeley'], \n",
    "                 ['output_McDonalds', 'output_BurgerKing', 'output_Starbucks', 'output_wendys'], \n",
    "                 ['output_IPL', 'output_MLB', 'output_MLS', 'output_NBA'], \n",
    "                 ['output_BBCWorld', 'output_CNN', 'output_washingtonpost', 'output_nytimes'], \n",
    "                 ['output_Netflix', 'output_Spotify', 'output_PrimeVideo', 'output_HBO'], \n",
    "                 ['output_amazon', 'output_facebook', 'output_Tesla', 'output_Microsoft']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800\n"
     ]
    }
   ],
   "source": [
    "sizeOfSets = 400\n",
    "\n",
    "categCount = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in subCategories[i]:\n",
    "        categCount += 1\n",
    "        \n",
    "categCount *= sizeOfSets\n",
    "\n",
    "val = [None] * categCount # Keep value (sizeOfSets * no. of categories)\n",
    "sizeOfTweetContent = 75\n",
    "\n",
    "print(len(val)) # No. Of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputDataInDataframe():\n",
    "    overCount= 0\n",
    "    for i in range(len(categories)):\n",
    "        for j in subCategories[i]:\n",
    "            direc:str = '/Users/sameernawaz/AnacondaProjects/Datasets/tweets/'+categories[i]+'/'+j+'.csv'\n",
    "            with open(direc, 'r') as csv_file:\n",
    "                csv_reader = csv.reader(csv_file)\n",
    "                count = 0\n",
    "                for line in csv_reader:\n",
    "                    if len(line[6]) > sizeOfTweetContent:\n",
    "                        val[overCount] = line\n",
    "                        count += 1\n",
    "                        overCount += 1\n",
    "                        if count >= sizeOfSets :\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataInDataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(val, columns = ['date', 'username', 'to', 'replies', \n",
    "                                  'retweets', 'favorites', 'text', 'geo', \n",
    "                                  'mentions', 'hashtags', 'id', 'permalink']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once data added in dataframe, Shuffle it\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "# Add a unique id (rec_id) for all tweets\n",
    "df = df.assign(rec_id=np.arange(len(df))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12800 entries, 0 to 12799\n",
      "Data columns (total 3 columns):\n",
      "rec_id      12800 non-null int64\n",
      "username    12800 non-null object\n",
      "text        12800 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 400.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df[['rec_id', 'username', 'text']]\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    doc = contractions.fix(doc)\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    #filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(list(df['text']))\n",
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12800, 26984)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12790</th>\n",
       "      <th>12791</th>\n",
       "      <th>12792</th>\n",
       "      <th>12793</th>\n",
       "      <th>12794</th>\n",
       "      <th>12795</th>\n",
       "      <th>12796</th>\n",
       "      <th>12797</th>\n",
       "      <th>12798</th>\n",
       "      <th>12799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.052041</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042221</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052041</td>\n",
       "      <td>0.013952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.018910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 12800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3      4      5         6      7      \\\n",
       "0  1.000000  0.013898  0.052041  0.025284    0.0    0.0  0.000000    0.0   \n",
       "1  0.013898  1.000000  0.013952  0.000000    0.0    0.0  0.000000    0.0   \n",
       "2  0.052041  0.013952  1.000000  0.010113    0.0    0.0  0.000000    0.0   \n",
       "3  0.025284  0.000000  0.010113  1.000000    0.0    0.0  0.041232    0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000    1.0    0.0  0.000000    0.0   \n",
       "\n",
       "      8         9      ...  12790     12791  12792     12793     12794  \\\n",
       "0  0.000000  0.014855  ...    0.0  0.009271    0.0  0.042221  0.025714   \n",
       "1  0.029604  0.000000  ...    0.0  0.013721    0.0  0.351401  0.000000   \n",
       "2  0.000000  0.000000  ...    0.0  0.012198    0.0  0.008266  0.018910   \n",
       "3  0.000000  0.038237  ...    0.0  0.013577    0.0  0.000000  0.000000   \n",
       "4  0.000000  0.000000  ...    0.0  0.000000    0.0  0.000000  0.000000   \n",
       "\n",
       "      12795  12796     12797  12798     12799  \n",
       "0  0.000000    0.0  0.008753    0.0  0.000000  \n",
       "1  0.013997    0.0  0.012955    0.0  0.019976  \n",
       "2  0.000000    0.0  0.000000    0.0  0.009926  \n",
       "3  0.000000    0.0  0.000000    0.0  0.000000  \n",
       "4  0.000000    0.0  0.000000    0.0  0.000000  \n",
       "\n",
       "[5 rows x 12800 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Pairwise Document Similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)\n",
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     1,     2, ..., 12797, 12798, 12799]), (12800,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of post IDs\n",
    "\n",
    "posts_list = df['rec_id'].values\n",
    "posts_list, posts_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3997"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_idx = np.where(posts_list == 3997)[0][0]\n",
    "posts_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_similarities = doc_sim_df.iloc[posts_idx].values\n",
    "posts_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12250, 10236,  9883,  6841,  1460])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 5 similar post IDs\n",
    "\n",
    "similar_posts_idxs = np.argsort(-posts_similarities)[1:6]\n",
    "similar_posts_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12250, 10236,  9883,  6841,  1460])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_posts = posts_list[similar_posts_idxs]\n",
    "similar_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_posts(postId, posts = posts_list, doc_sims = doc_sim_df):\n",
    "    # find post id\n",
    "    posts_idx = np.where(posts == postId)[0][0]\n",
    "    # get posts similarities\n",
    "    posts_similarities = doc_sims.iloc[posts_idx].values\n",
    "    # get top 5 similar post IDs\n",
    "    similar_posts_idxs = np.argsort(-posts_similarities)[1:6]\n",
    "    # get top 5 posts\n",
    "    similar_posts = posts[similar_posts_idxs]\n",
    "    # return the top 5 posts\n",
    "    return similar_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Input Post-----\n",
      "An algorithm developed at @MIT_CSAIL could help doctors better analyze the placenta and monitor for potential issues during pregnancy. via @engadget\n",
      "\n",
      "-----Relevant Posts-----\n",
      "=> An algorithm developed to study the structure of galaxies helps explain a key feature of embryonic development. http://mitsha.re/SgsM50vjL4N\n",
      "=> The #HoloLens2 is better than ever and is now shipping to customers. Get to know the mixed reality headset: http://msft.social/pqlAYR via @engadget\n",
      "=> Mechanical engineers have developed a steerable, thread-like robot that can actively glide through narrow, winding blood vessels. This robotic thread could help doctors treat blockages and lesions that occur in aneurysms and stroke. Video by Melanie Gonick/MIT\n",
      "=> #LegenDMB @DaMarcusBeasley wants to help the potential DaMarcus Beasleys of tomorrow.\n",
      "=> This advance may one day help doctors treat patients with traumatic #brain injuries, strokes and aneurysms. #BerkeleyResearch\n"
     ]
    }
   ],
   "source": [
    "post_id = 1059\n",
    "\n",
    "print('-----Input Post-----')\n",
    "print((df.loc[df['rec_id'] == post_id]['text']).values[0])\n",
    "print('\\n-----Relevant Posts-----')\n",
    "for i in relevant_posts(postId = post_id, posts = posts_list, doc_sims = doc_sim_df):\n",
    "    print(\"=>\",(df.loc[df['rec_id'] == i]['text']).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
